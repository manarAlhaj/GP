from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader
import torch
import torch.nn as nn
import torch.optim as optim

import pandas as pd #get the dataset as pandas
from datasets import Dataset #make the pandas set a huggiingface set why? bc it has ttokenrizers and trainers
import string
import torchtext
df = pd.read_excel("dataset.xlsx") #got our dataset

dataset = Dataset.from_pandas(df) #make it a huggingface dataset

#now we are gonna need to split the dattaset
split_dataset = dataset.train_test_split(test_size=0.1) #the rows will be shuffeled dont be alarmed

train_ds = split_dataset["train"]
test_ds = split_dataset["test"]


#get a spacy model that can deal wiiith arabic to get a tokenizer

import regex as re
import string

sos_token = "<sos>"
eos_token = "<eos>"
max_length = 20

punctuation = string.punctuation + "؟،؛"

# Arabic letters + diacritics
arabic_pattern = r'[\p{Arabic}\p{M}]+|[a-zA-Z0-9]+'
# \p{M} matches **marks** (diacritics)

def clean_text(text):
    # remove punctuation
    return ''.join([c for c in str(text) if c not in punctuation])

def tokenize_example(example):
    src_clean = clean_text(example["sign"])
    src_tokens = re.findall(arabic_pattern, src_clean)[:max_length]

    tgt_clean = clean_text(example["spoken"])
    tgt_tokens = re.findall(arabic_pattern, tgt_clean)[:max_length-2]
    tgt_tokens = [sos_token] + tgt_tokens + [eos_token]

    return {
        "sign_tokens": src_tokens,
        "spoken_tokens": tgt_tokens
    }


# Apply to your dataset
train_token = train_ds.map(tokenize_example) #both of this these bad boys now have 4 columns each column wth its token
test_token = test_ds.map(tokenize_example)
print(train_token[0])
 
 
 #now lets build tthe vocab by assignning a numbe to each word
unk_token = "<unk>"
pad_token = "<pad>"
sos_token = "<sos>"
eos_token = "<eos>"

special_tokens = [unk_token, pad_token, sos_token, eos_token]
min_freq = 1


# Source vocabulary (sign language)
sign_vocab = torchtext.vocab.build_vocab_from_iterator(
    train_token["sign_tokens"],  # use tokenized version
    min_freq=min_freq,
    specials=special_tokens
)

spoken_vocab = torchtext.vocab.build_vocab_from_iterator(
    train_token["spoken_tokens"],
    min_freq=min_freq,
    specials=special_tokens
)



print(sign_vocab['امتحان'])   # integer index of 'تروح'
print(spoken_vocab['<sos>'])  # index of <sos>

#numericalize
# Set default index to <unk> for unknown tokens
sign_vocab.set_default_index(sign_vocab["<unk>"])
spoken_vocab.set_default_index(spoken_vocab["<unk>"])

# Function to convert tokens into indices
def numericalize_example(example, sign_vocab, spoken_vocab):
    sign_ids = sign_vocab.lookup_indices(example["sign_tokens"])
    spoken_ids = spoken_vocab.lookup_indices(example["spoken_tokens"])
    return {"sign_ids": sign_ids, "spoken_ids": spoken_ids}


# Apply to your dataset
train_num = train_token.map(numericalize_example, fn_kwargs={"sign_vocab": sign_vocab, "spoken_vocab": spoken_vocab})
test_num = test_token.map(
    numericalize_example,
    fn_kwargs={"sign_vocab": sign_vocab, "spoken_vocab": spoken_vocab}
)


# Check the first example
print(train_num[0])



def collate_batch(batch):
    sign_batch = [torch.tensor(x["sign_ids"]) for x in batch]
    spoken_batch = [torch.tensor(x["spoken_ids"]) for x in batch]

    sign_batch = pad_sequence(sign_batch, batch_first=True, padding_value=sign_vocab["<pad>"])
    spoken_batch = pad_sequence(spoken_batch, batch_first=True, padding_value=spoken_vocab["<pad>"])

    return sign_batch, spoken_batch

#hf formatting fpr torch

train_loader = DataLoader(train_num, batch_size=32, shuffle=True, collate_fn=collate_batch)
test_loader = DataLoader(test_num, batch_size=32, collate_fn=collate_batch)



class Encoder(nn.Module):
    def __init__(self, input_dim, emb_dim, hidden_dim, num_layers=1, dropout=0.1):
        super(Encoder, self).__init__()
        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=sign_vocab["<pad>"])
        self.gru = nn.GRU(emb_dim, hidden_dim, num_layers=num_layers, batch_first=True,
                          dropout=dropout if num_layers > 1 else 0)
        self.hidden_dim = hidden_dim

    def forward(self, src):
        embedded = self.embedding(src)
        outputs, hidden = self.gru(embedded)
        return hidden



class Decoder(nn.Module):
    def __init__(self, output_dim, emb_dim, hidden_dim, num_layers=1, dropout=0.1):
        super(Decoder, self).__init__()
        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=spoken_vocab["<pad>"])
        self.gru = nn.GRU(emb_dim, hidden_dim, num_layers=num_layers, batch_first=True,
                          dropout=dropout if num_layers > 1 else 0)
        self.fc_out = nn.Linear(hidden_dim, output_dim)
        self.hidden_dim = hidden_dim

    def forward(self, input, hidden):
        input = input.unsqueeze(1)  # shape: [batch_size, 1]
        embedded = self.embedding(input)
        output, hidden = self.gru(embedded, hidden)
        prediction = self.fc_out(output.squeeze(1))
        return prediction, hidden


class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super(Seq2Seq, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device

    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        batch_size = src.size(0)
        trg_len = trg.size(1)
        trg_vocab_size = self.decoder.embedding.num_embeddings

        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)
        hidden = self.encoder(src)

        input = trg[:, 0]  # first token is <sos>

        for t in range(1, trg_len):
            output, hidden = self.decoder(input, hidden)
            outputs[:, t] = output
            teacher_force = torch.rand(1).item() < teacher_forcing_ratio
            top1 = output.argmax(1)
            input = trg[:, t] if teacher_force else top1

        return outputs


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

INPUT_DIM = len(sign_vocab)
OUTPUT_DIM = len(spoken_vocab)
EMB_DIM = 128
HIDDEN_DIM = 256

encoder = Encoder(INPUT_DIM, EMB_DIM, HIDDEN_DIM).to(device)
decoder = Decoder(OUTPUT_DIM, EMB_DIM, HIDDEN_DIM).to(device)
model = Seq2Seq(encoder, decoder, device).to(device)

optimizer = optim.Adam(model.parameters())
criterion = nn.CrossEntropyLoss(ignore_index=spoken_vocab["<pad>"])


num_epochs = 10

def translate_sentence(model, src_tensor, sign_vocab, spoken_vocab, max_len=20):
    model.eval()
    src_tensor = src_tensor.unsqueeze(0).to(model.device)
    hidden = model.encoder(src_tensor)

    input_token = torch.tensor([spoken_vocab["<sos>"]]).to(model.device)
    outputs = []

    for _ in range(max_len):
        output, hidden = model.decoder(input_token, hidden)
        top1 = output.argmax(1).item()
        if top1 == spoken_vocab["<eos>"]:
            break
        outputs.append(top1)
        input_token = torch.tensor([top1]).to(model.device)

    idx_to_word = {idx: word for word, idx in spoken_vocab.get_stoi().items()}
    translated = [idx_to_word.get(idx, "<unk>") for idx in outputs]
    return " ".join(translated)

for epoch in range(num_epochs):
    model.train()
    epoch_loss = 0

    for src_batch, trg_batch in train_loader:
        src_batch = src_batch.to(device)
        trg_batch = trg_batch.to(device)

        optimizer.zero_grad()
        output = model(src_batch, trg_batch)

        output_dim = output.shape[-1]
        output = output[:, 1:].reshape(-1, output_dim)
        trg = trg_batch[:, 1:].reshape(-1)

        loss = criterion(output, trg)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()

    print(f"Epoch {epoch+1} | Loss: {epoch_loss/len(train_loader):.4f}")

    # --- Print some test examples after each epoch ---
    model.eval()
    with torch.no_grad():
        for src_batch, trg_batch in test_loader:
            for i in range(min(3, src_batch.size(0))):  # show 3 examples
                src_seq = src_batch[i]
                trg_seq = trg_batch[i]

                prediction = translate_sentence(model, src_seq, sign_vocab, spoken_vocab)
                actual = [spoken_vocab.lookup_token(idx.item()) 
                          for idx in trg_seq if idx.item() not in (spoken_vocab["<pad>"], spoken_vocab["<sos>"], spoken_vocab["<eos>"])]

                print(f"Sign input: {[sign_vocab.lookup_token(idx.item()) for idx in src_seq if idx.item() != sign_vocab['<pad>']]}")
                print(f"Actual spoken: {' '.join(actual)}")
                print(f"Predicted spoken: {prediction}\n")
            break  # just first batch
